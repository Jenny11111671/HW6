{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12c8670",
   "metadata": {},
   "source": [
    " Explain the theoretical Simple Linear Regression model in your own words by describing its components (of predictor and outcome variables, slope and intercept coefficients, and an error term) and how they combine to form a sample from normal distribution; then, create python code explicitly demonstrating your explanation using numpy and scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b47e2",
   "metadata": {},
   "source": [
    "The theoretical Simple Linear Regression (SLR) model aims to predict an outcome variable, often called the dependent or response variable \n",
    "Y\n",
    "Y, based on a single predictor variable \n",
    "X\n",
    "X, also known as the independent variable. In its simplest form, SLR tries to establish a linear relationship between \n",
    "X\n",
    "X and \n",
    "Y\n",
    "Y by describing \n",
    "Y\n",
    "Y as a function of \n",
    "X\n",
    "X.\n",
    "\n",
    "Components of the SLR Model:\n",
    "Predictor Variable (\n",
    "X\n",
    "X): This is the independent variable that we use to predict or explain changes in \n",
    "Y\n",
    "Y.\n",
    "Outcome Variable (\n",
    "Y\n",
    "Y): This is the dependent variable, the one we’re trying to predict or model using \n",
    "X\n",
    "X.\n",
    "Intercept (\n",
    "β\n",
    "0\n",
    "β \n",
    "0\n",
    "​\t\n",
    " ): The intercept is the value of \n",
    "Y\n",
    "Y when \n",
    "X\n",
    "X is zero. It represents the starting point of the line on the \n",
    "Y\n",
    "Y-axis in the absence of any influence from \n",
    "X\n",
    "X.\n",
    "Slope (\n",
    "β\n",
    "1\n",
    "β \n",
    "1\n",
    "​\t\n",
    " ): The slope represents the expected change in \n",
    "Y\n",
    "Y for a one-unit change in \n",
    "X\n",
    "X. It’s the rate of increase or decrease in \n",
    "Y\n",
    "Y relative to \n",
    "X\n",
    "X.\n",
    "Error Term (\n",
    "ϵ\n",
    "ϵ): The error term captures the variability in \n",
    "Y\n",
    "Y that cannot be explained by the linear relationship with \n",
    "X\n",
    "X. It represents random deviations from the line defined by \n",
    "β\n",
    "0\n",
    "+\n",
    "β\n",
    "1\n",
    "X\n",
    "β \n",
    "0\n",
    "​\t\n",
    " +β \n",
    "1\n",
    "​\t\n",
    " X.\n",
    "Normal Distribution: We assume that the errors \n",
    "ϵ\n",
    "ϵ are normally distributed with a mean of 0 and constant variance \n",
    "σ\n",
    "2\n",
    "σ \n",
    "2\n",
    " . This normality assumption makes it possible to use probability distributions to describe the variability of \n",
    "Y\n",
    "Y around its expected value.\n",
    "The regression model is expressed as:\n",
    "\n",
    "Y\n",
    "=\n",
    "β\n",
    "0\n",
    "+\n",
    "β\n",
    "1\n",
    "X\n",
    "+\n",
    "ϵ\n",
    "Y=β \n",
    "0\n",
    "​\t\n",
    " +β \n",
    "1\n",
    "​\t\n",
    " X+ϵ\n",
    "where \n",
    "ϵ\n",
    "∼\n",
    "N\n",
    "(\n",
    "0\n",
    ",\n",
    "σ\n",
    "2\n",
    ")\n",
    "ϵ∼N(0,σ \n",
    "2\n",
    " ).\n",
    "\n",
    "Combining the Components:\n",
    "This equation says that \n",
    "Y\n",
    "Y is modeled as a linear function of \n",
    "X\n",
    "X, with some random noise added to account for the fact that not all the variation in \n",
    "Y\n",
    "Y can be explained by \n",
    "X\n",
    "X alone. Given that \n",
    "ϵ\n",
    "ϵ is normally distributed, the resulting values of \n",
    "Y\n",
    "Y are also normally distributed for any fixed value of\n",
    "X\n",
    "X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1215332",
   "metadata": {},
   "source": [
    "Python Code to Demonstrate the SLR Model\n",
    "Here's how we can simulate the components of a simple linear regression model using numpy and scipy.stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf96e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "beta_0 = 3.0  # Intercept\n",
    "beta_1 = 1.5  # Slope\n",
    "sigma = 1.0   # Standard deviation of the error term\n",
    "\n",
    "# Generate a sample of predictor variable X\n",
    "n_samples = 100\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Generate normally distributed error term with mean 0 and standard deviation sigma\n",
    "error = np.random.normal(0, sigma, n_samples)\n",
    "\n",
    "# Calculate the outcome variable Y based on the SLR model\n",
    "Y = beta_0 + beta_1 * X + error\n",
    "\n",
    "# Plot the simulated data points and the true regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, Y, label='Observed Data (Y)', color='blue', alpha=0.6)\n",
    "plt.plot(X, beta_0 + beta_1 * X, label='True Regression Line (without error)', color='red')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.title('Simple Linear Regression Model Demonstration')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Display theoretical normal distribution for one example X value (e.g., X=5)\n",
    "x_value = 5\n",
    "mean_y = beta_0 + beta_1 * x_value\n",
    "y_distribution = norm(loc=mean_y, scale=sigma)\n",
    "\n",
    "# Generate Y values for this specific X value\n",
    "y_values = np.linspace(mean_y - 3*sigma, mean_y + 3*sigma, 100)\n",
    "pdf_values = y_distribution.pdf(y_values)\n",
    "\n",
    "# Plot the probability density for Y at X=5\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_values, pdf_values, label=f\"Distribution of Y | X={x_value}\")\n",
    "plt.xlabel(\"Y values\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(f\"Normal Distribution of Y at X={x_value}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72697269",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "Data Generation: We set up \n",
    "X\n",
    "X as a linearly spaced array. The outcome variable \n",
    "Y\n",
    "Y is calculated using the formula \n",
    "Y\n",
    "=\n",
    "β\n",
    "0\n",
    "+\n",
    "β\n",
    "1\n",
    "×\n",
    "X\n",
    "+\n",
    "ϵ\n",
    "Y=β \n",
    "0\n",
    "​\t\n",
    " +β \n",
    "1\n",
    "​\t\n",
    " ×X+ϵ, where \n",
    "ϵ\n",
    "ϵ is randomly sampled from a normal distribution with mean 0 and standard deviation \n",
    "σ\n",
    "σ.\n",
    "Plotting the Data: The first plot shows the observed data points (in blue) and the true regression line without noise (in red).\n",
    "Distribution Plot: To illustrate that \n",
    "Y\n",
    "Y follows a normal distribution for a fixed \n",
    "X\n",
    "X, we generate and plot the probability density function (PDF) of \n",
    "Y\n",
    "Y at \n",
    "X\n",
    "=\n",
    "5\n",
    "X=5, using the theoretical mean and variance for that point.\n",
    "This code demonstrates how the SLR model combines the predictor variable, slope, intercept, and error term to create normally distributed values for \n",
    "Y\n",
    "Y around the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871826c9",
   "metadata": {},
   "source": [
    " Use a dataset simulated from your theoretical Simple Linear Regression model to demonstrate how to create and visualize a fitted Simple Linear Regression model using pandas and import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ea1b08",
   "metadata": {},
   "source": [
    "Let's create a synthetic dataset using the Simple Linear Regression (SLR) model and then use pandas to organize the data and statsmodels to fit a linear regression model. We’ll visualize the fitted regression line along with the observed data points to illustrate how well the model fits.\n",
    "\n",
    "Here's the Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cb589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "beta_0 = 3.0  # Intercept\n",
    "beta_1 = 1.5  # Slope\n",
    "sigma = 1.0   # Standard deviation of the error term\n",
    "\n",
    "# Generate a sample of predictor variable X\n",
    "n_samples = 100\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Generate normally distributed error term\n",
    "error = np.random.normal(0, sigma, n_samples)\n",
    "\n",
    "# Calculate the outcome variable Y based on the SLR model\n",
    "Y = beta_0 + beta_1 * X + error\n",
    "\n",
    "# Create a DataFrame from the simulated data\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Fit the simple linear regression model using statsmodels\n",
    "model = smf.ols('Y ~ X', data=data).fit()\n",
    "\n",
    "# Display model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Extract predicted values from the fitted model\n",
    "data['Y_pred'] = model.predict(data['X'])\n",
    "\n",
    "# Plot the observed data points and the fitted regression line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['Y'], label='Observed Data (Y)', color='blue', alpha=0.6)\n",
    "plt.plot(data['X'], data['Y_pred'], label='Fitted Regression Line', color='red')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.title('Fitted Simple Linear Regression Model')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03793",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "Data Simulation: We generate a dataset using the theoretical SLR model, where \n",
    "Y\n",
    "=\n",
    "β\n",
    "0\n",
    "+\n",
    "β\n",
    "1\n",
    "⋅\n",
    "X\n",
    "+\n",
    "ϵ\n",
    "Y=β \n",
    "0\n",
    "​\t\n",
    " +β \n",
    "1\n",
    "​\t\n",
    " ⋅X+ϵ, with \n",
    "X\n",
    "X as the predictor variable, and \n",
    "ϵ\n",
    "ϵ as the normally distributed error term.\n",
    "DataFrame Creation: We load the generated data into a pandas DataFrame, making it easy to work with in statsmodels.\n",
    "Model Fitting: Using statsmodels.formula.api.ols, we fit a linear regression model to predict \n",
    "Y\n",
    "Y based on \n",
    "X\n",
    "X. The summary() output provides key statistics on the model fit, including coefficients, R-squared, and p-values.\n",
    "Visualization: We plot the observed data points and overlay the fitted regression line, helping to visualize the model’s fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4c2bc",
   "metadata": {},
   "source": [
    "3.Add the line from Question 1 on the figure of Question 2 and explain the difference between the nature of the two lines in your own words; but, hint though: simulation of random sampling variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac10d3d",
   "metadata": {},
   "source": [
    "The purpose of adding the original theoretical line (from Question 1) to the fitted model line (from Question 2) is to illustrate the difference between the theoretical relationship we define for \n",
    "Y\n",
    "Y in the model, and the observed relationship derived from the data sample. This comparison visually highlights the effects of random sampling variation on model estimation, which occurs because each data sample includes a different random error component \n",
    "ϵ\n",
    "ϵ.\n",
    "\n",
    "In other words, the two lines differ due to the fact that, while the theoretical line represents the exact relationship we assume in the model (i.e., the population relationship), the fitted line is based on observed data points, which can vary due to random noise. By repeating simulations, we would see that the fitted line changes slightly with each sample, while the theoretical line remains the same. This helps to illustrate that the fitted line is an estimate that becomes more accurate as sample size grows but will always contain some degree of sampling variation.\n",
    "\n",
    "Let's add the theoretical line to the fitted plot and visualize the differences:\n",
    "\n",
    "Here’s the updated Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define parameters\n",
    "beta_0 = 3.0  # Intercept\n",
    "beta_1 = 1.5  # Slope\n",
    "sigma = 1.0   # Standard deviation of the error term\n",
    "\n",
    "# Generate a sample of predictor variable X\n",
    "n_samples = 100\n",
    "X = np.linspace(0, 10, n_samples)\n",
    "\n",
    "# Generate normally distributed error term\n",
    "error = np.random.normal(0, sigma, n_samples)\n",
    "\n",
    "# Calculate the outcome variable Y based on the SLR model\n",
    "Y = beta_0 + beta_1 * X + error\n",
    "\n",
    "# Create a DataFrame from the simulated data\n",
    "data = pd.DataFrame({'X': X, 'Y': Y})\n",
    "\n",
    "# Fit the simple linear regression model using statsmodels\n",
    "model = smf.ols('Y ~ X', data=data).fit()\n",
    "\n",
    "# Display model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Extract predicted values from the fitted model\n",
    "data['Y_pred'] = model.predict(data['X'])\n",
    "\n",
    "# Define the theoretical line (without random error)\n",
    "x_range = np.array([data['X'].min(), data['X'].max()])\n",
    "y_line = beta_0 + beta_1 * x_range  # theoretical line\n",
    "\n",
    "# Plot the observed data points, fitted regression line, and theoretical line\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data['X'], data['Y'], label='Observed Data (Y)', color='blue', alpha=0.6)\n",
    "plt.plot(data['X'], data['Y_pred'], label='Fitted Regression Line', color='red')\n",
    "plt.plot(x_range, y_line, label='Theoretical Line (3 + 1.5 * X)', color='orange', linestyle='--')\n",
    "plt.xlabel('Predictor Variable (X)')\n",
    "plt.ylabel('Outcome Variable (Y)')\n",
    "plt.title('Comparison of Fitted Line and Theoretical Line in Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ab35f",
   "metadata": {},
   "source": [
    "Explanation of the Graph\n",
    "Fitted Line (Red): This line is the result of the model fit using the observed data. It represents the best estimate of the relationship between \n",
    "X\n",
    "X and \n",
    "Y\n",
    "Y based on the sample data, accounting for random error in the dataset.\n",
    "Theoretical Line (Orange, Dashed): This line is based on the exact values of \n",
    "β\n",
    "0\n",
    "β \n",
    "0\n",
    "​\t\n",
    "  and \n",
    "β\n",
    "1\n",
    "β \n",
    "1\n",
    "​\t\n",
    "  used in the data generation process, representing the true underlying relationship between \n",
    "X\n",
    "X and\n",
    "Y\n",
    "Y (without error). It is unaffected by sampling variability.\n",
    "Key Insight\n",
    "By comparing these lines, we see that:\n",
    "\n",
    "The fitted line approximates the theoretical line but varies slightly because of random noise in each sample. If we repeated this process (re-running the cell with fresh samples), each fitted line would fluctuate around the theoretical line, sometimes closer or farther away.\n",
    "Sampling Variation: Each dataset sampled introduces unique random errors, causing slight changes in the fitted line. Over many simulations, these variations average out, and the fitted line converges toward the theoretical line, reflecting the concept of sampling distribution and unbiased estimators in regression.\n",
    "This demonstration emphasizes how sampling variability influences regression estimates and why large samples tend to yield more accurate and stable fitted lines.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01323286",
   "metadata": {},
   "source": [
    "4. Explain how fitted_model.fittedvalues are derived on the basis of fitted_model.summary().tables[1] (or more specifically fitted_model.params or fitted_model.params.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6295",
   "metadata": {},
   "source": [
    "For question 4, here is an explanation of how `fitted_model.fittedvalues` are derived based on `fitted_model.params`:\n",
    "\n",
    "In the Simple Linear Regression model, the fitted values (\\( \\hat{Y}_i \\)) are predictions made for each observed value of \\( Y \\) based on the estimated regression line. This line is defined by the estimated intercept \\( \\hat{\\beta}_0 \\) and slope \\( \\hat{\\beta}_1 \\), which are obtained from the sample data through the ordinary least squares (OLS) estimation method.\n",
    "\n",
    "The fitted values can be expressed as:\n",
    "\\[\n",
    "\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\n",
    "\\]\n",
    "where:\n",
    "- \\( \\hat{\\beta}_0 \\) and \\( \\hat{\\beta}_1 \\) are the estimated intercept and slope, respectively. These are calculated from the data and stored in `fitted_model.params`.\n",
    "- \\( X_i \\) represents the predictor variable values for each observation.\n",
    "\n",
    "The process to calculate `fitted_model.fittedvalues` involves taking each \\( X_i \\) from the data, plugging it into the estimated regression equation using the parameters in `fitted_model.params`, and computing the corresponding \\( \\hat{Y}_i \\).\n",
    "\n",
    "In summary:\n",
    "1. `fitted_model.params` contains the estimated parameters \\( \\hat{\\beta}_0 \\) (intercept) and \\( \\hat{\\beta}_1 \\) (slope) obtained from fitting the model to the data.\n",
    "2. `fitted_model.fittedvalues` are generated by substituting these parameter values into the regression equation for each observed \\( X_i \\), providing the in-sample predictions for \\( Y \\).\n",
    "\n",
    "The fitted values are essentially the best linear approximation of \\( Y \\) given the sample data, and they form the basis for calculating residuals (the differences between observed and fitted values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f6e97",
   "metadata": {},
   "source": [
    "5. Explain concisely in your own words what line is chosen for the fitted model based on observed data using the \"ordinary least squares\" method (as is done by trendline='ols' and smf.ols(...).fit()) and why it requires \"squares\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eed861",
   "metadata": {},
   "source": [
    "In the \"ordinary least squares\" (OLS) method, the fitted line chosen for the model is the one that minimizes the sum of the squared differences between the observed values \\( Y_i \\) and the predicted values \\( \\hat{Y}_i \\) based on the line. These differences are called **residuals** (\\( e_i = Y_i - \\hat{Y}_i \\)).\n",
    "\n",
    "The reason OLS uses **squares** is to ensure that both positive and negative residuals contribute equally to the error measure. Squaring each residual avoids the issue of positive and negative values canceling each other out, and it emphasizes larger errors, making the line fit in a way that minimizes these larger discrepancies. By minimizing the sum of squared residuals, OLS provides the \"best-fit\" line that most closely aligns with the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa0e95",
   "metadata": {},
   "source": [
    "6. Explain why the first expression below can be interpreted as \"the proportion of variation in (outcome) Y explained by the model (i.e. fitted_model.fittedvalues)\"; and therefore, why fitted_model.rsquared can be interpreted as a measure of the accuracy of the model; and, therefore what the two np.corrcoef(...)[0,1]**2 expressions capture in the context of Simple Linear Regression models.\n",
    "\n",
    "1-((Y-fitted_model.fittedvalues)**2).sum()/((Y-Y.mean())**2).sum()\n",
    "fitted_model.rsquared\n",
    "np.corrcoef(Y,fitted_model.fittedvalues)[0,1]**2\n",
    "np.corrcoef(Y,x)[0,1]**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae62c1",
   "metadata": {},
   "source": [
    "In Simple Linear Regression, these expressions relate to how well the model explains the variation in the outcome variable \\( Y \\). Here’s a breakdown of each part:\n",
    "\n",
    "### 1. The Expression \\( 1 - \\frac{\\sum (Y - \\hat{Y})^2}{\\sum (Y - \\bar{Y})^2} \\)\n",
    "\n",
    "This expression calculates the **proportion of variation in \\( Y \\) explained by the model**. Here’s why:\n",
    "- \\( \\sum (Y - \\bar{Y})^2 \\): This is the **total variation** in \\( Y \\), representing how much \\( Y \\) varies around its mean. It's the **total sum of squares** (TSS).\n",
    "- \\( \\sum (Y - \\hat{Y})^2 \\): This is the **unexplained variation** in \\( Y \\) after fitting the model, representing how much \\( Y \\) differs from its fitted values. It's the **residual sum of squares** (RSS).\n",
    "\n",
    "The expression \\( 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\) tells us the proportion of the total variation that is explained by the model. If this proportion is high, it indicates that the model explains a large part of the variation in \\( Y \\), meaning a better fit.\n",
    "\n",
    "### 2. `fitted_model.rsquared`\n",
    "\n",
    "The R-squared value, accessible via `fitted_model.rsquared`, is a direct calculation of the above expression and is a standard measure of **model accuracy**. R-squared ranges from 0 to 1:\n",
    "- **R-squared = 1**: The model perfectly explains all the variation in \\( Y \\).\n",
    "- **R-squared = 0**: The model explains none of the variation in \\( Y \\); it’s as good as the mean.\n",
    "\n",
    "An R-squared value closer to 1 indicates a more accurate model in terms of capturing the variability in \\( Y \\).\n",
    "\n",
    "### 3. `np.corrcoef(Y, fitted_model.fittedvalues)[0,1]**2`\n",
    "\n",
    "This expression calculates the **square of the correlation coefficient** between the observed \\( Y \\) values and the fitted \\( Y \\) values. In the context of linear regression, it’s mathematically equivalent to R-squared:\n",
    "- The correlation coefficient \\( r \\) measures the **linear association** between \\( Y \\) and \\( \\hat{Y} \\).\n",
    "- Squaring \\( r \\) gives \\( r^2 \\), the proportion of variance in \\( Y \\) explained by \\( \\hat{Y} \\).\n",
    "\n",
    "This squared correlation thus captures how well the fitted values approximate the observed values in the linear model.\n",
    "\n",
    "### 4. `np.corrcoef(Y, X)[0,1]**2`\n",
    "\n",
    "In the context of Simple Linear Regression, this captures the **proportion of variance in \\( Y \\) that is linearly associated with \\( X \\)**, as \\( X \\) is the predictor variable used to generate \\( \\hat{Y} \\). Because we’re only using one predictor, \\( r^2 \\) between \\( X \\) and \\( Y \\) is also equal to the model’s R-squared value.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **R-squared** (and these equivalent expressions) measures how much of the variation in \\( Y \\) is explained by the linear model.\n",
    "- A higher R-squared means the model is more accurate in explaining \\( Y \\) based on \\( X \\).\n",
    "- **In the context of Simple Linear Regression**, the expressions \\( 1 - \\frac{\\text{RSS}}{\\text{TSS}} \\), `fitted_model.rsquared`, and `np.corrcoef(Y, fitted_model.fittedvalues)[0,1]**2` all capture this concept of explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39a739",
   "metadata": {},
   "source": [
    "7. Indicate a couple of the assumptions of the Simple Linear Regression model specification that do not seem compatible with the example data below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a868e",
   "metadata": {},
   "source": [
    "In Simple Linear Regression, several key assumptions need to hold for the model to be appropriate and produce reliable results. Based on the given data and residuals, here are a couple of assumptions that might not be compatible with this dataset:\n",
    "\n",
    "### 1. **Linearity Assumption**\n",
    "\n",
    "The Simple Linear Regression model assumes a **linear relationship** between the predictor variable (amount of fertilizer) and the outcome variable (crop yield). In the scatter plot of crop yield versus fertilizer, however, it appears that the relationship might not be strictly linear. There could be a **non-linear trend** in the data, as crop yield appears to increase at a faster rate as the amount of fertilizer increases, especially at higher values. This pattern suggests a potential curvilinear relationship, which violates the linearity assumption.\n",
    "\n",
    "### 2. **Normality of Residuals**\n",
    "\n",
    "The Simple Linear Regression model assumes that the **residuals are normally distributed** with a mean of zero. In the histogram of residuals, if we observe a non-normal distribution (e.g., skewed, with outliers, or not symmetric), it suggests that this assumption may be violated. Based on the description, the residuals might not be normally distributed, indicating that the model is not capturing the pattern of the data accurately, which could be due to the non-linear nature of the relationship.\n",
    "\n",
    "### Explanation of the Two Violations\n",
    "\n",
    "These two issues suggest that a linear model may not be the best fit for this data. The non-linear relationship implies that a polynomial regression or other non-linear model could better capture the association between fertilizer amount and crop yield. Additionally, if the residuals are not normally distributed, it suggests that the error terms are not behaving as expected under the linear model assumptions, which can lead to unreliable parameter estimates and predictions. \n",
    "\n",
    "Overall, these potential violations indicate that alternative models or transformations might be more suitable for capturing the relationship in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8bb1a",
   "metadata": {},
   "source": [
    "8. Specify a null hypothesis of \"no linear association (on average)\" in terms of the relevant parameter of the Simple Linear Regression model, and use the code below to characterize the evidence in the data relative to the null hypothesis and interpret your subsequent beliefs regarding the Old Faithful Geyser dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "To analyze the Old Faithful Geyser dataset, let's specify a null hypothesis of \"no linear association (on average)\" between the waiting time (the predictor variable) and the eruption duration (the outcome variable).\n",
    "\n",
    "Null and Alternative Hypotheses\n",
    "In terms of the Simple Linear Regression model:\n",
    "\n",
    "Null Hypothesis (\n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " ): The slope parameter, \n",
    "β\n",
    "1\n",
    "β \n",
    "1\n",
    "​\t\n",
    " , is zero. This implies that there is no linear association between the waiting time and the eruption duration, meaning changes in waiting time do not affect eruption duration.\n",
    "H\n",
    "0\n",
    ":\n",
    "β\n",
    "1\n",
    "=\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " :β \n",
    "1\n",
    "​\t\n",
    " =0\n",
    "Alternative Hypothesis (\n",
    "H\n",
    "1\n",
    "H \n",
    "1\n",
    "​\t\n",
    " ): The slope parameter, \n",
    "β\n",
    "1\n",
    "β \n",
    "1\n",
    "​\t\n",
    " , is not zero. This implies a linear association between the waiting time and eruption duration, meaning that changes in waiting time are associated with changes in eruption duration.\n",
    "H\n",
    "1\n",
    ":\n",
    "β\n",
    "1\n",
    "≠\n",
    "0\n",
    "H \n",
    "1\n",
    "​\t\n",
    " :β \n",
    "1\n",
    "​\t\n",
    " \n",
    "\n",
    "=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a475b4",
   "metadata": {},
   "source": [
    "Code to Fit the Model and View the Results\n",
    "The following code loads the Old Faithful Geyser dataset, specifies the linear regression model with waiting as the predictor and duration as the outcome, and fits the model using OLS. We then check the p-value for the slope to determine the strength of evidence against the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6628e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load the \"Classic\" Old Faithful Geyser dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Specify the linear model for analysis\n",
    "linear_for_specification = 'duration ~ waiting'\n",
    "model = smf.ols(linear_for_specification, data=old_faithful)\n",
    "\n",
    "# Fit the model\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Display the summary of the fitted model\n",
    "print(fitted_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffbc74",
   "metadata": {},
   "source": [
    "Interpreting the Output\n",
    "In the output from fitted_model.summary(), focus on the p-value associated with the waiting variable (the slope). This p-value tells us the strength of evidence against the null hypothesis \n",
    "H\n",
    "0\n",
    ":\n",
    "β\n",
    "1\n",
    "=\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " :β \n",
    "1\n",
    "​\t\n",
    " =0.\n",
    "\n",
    "Interpretation Based on P-value\n",
    "\n",
    "If the p-value is very low (e.g., < 0.001): This would indicate very strong evidence against the null hypothesis. We would reject \n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " , suggesting a significant linear association between waiting time and eruption duration.\n",
    "If the p-value is between 0.01 and 0.05: This would indicate moderate to strong evidence against the null hypothesis, allowing us to reject \n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " .\n",
    "If the p-value is between 0.05 and 0.1: This would indicate weak evidence against the null hypothesis, suggesting a possible linear association but not enough evidence to confidently reject \n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " .\n",
    "If the p-value is greater than 0.1: This would indicate no evidence against the null hypothesis, so we would fail to reject \n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " , suggesting no significant linear association between waiting time and eruption duration.\n",
    "Conclusion\n",
    "Based on the p-value from the fitted model:\n",
    "\n",
    "If we reject the null hypothesis, we conclude that there is evidence of a linear association between waiting time and eruption duration for Old Faithful.\n",
    "If we fail to reject the null hypothesis, we conclude that the data do not provide enough evidence to claim a linear relationship between these variables.\n",
    "This approach allows us to make an informed decision about the presence or absence of a linear association without assuming that our conclusion \"proves\" the relationship; it simply provides evidence for or against it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d42f4",
   "metadata": {},
   "source": [
    "9. As seen in the introductory figure above, if the delay of the geyser eruption since the previous geyser eruption exceeds approximately 63 minutes, there is a notable increase in the duration of the geyser eruption itself. In the figure below we therefore restrict the dataset to only short wait times. Within the context of only short wait times, is there evidence in the data for a relationship between duration and wait time in the same manner as in the full data set? Using the following code, characterize the evidence against the null hypothesis in the context of short wait times which are less than short_wait_limit values of 62, 64, 66."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26c7c5",
   "metadata": {},
   "source": [
    "To address this question, we analyze whether there is evidence of a relationship between geyser eruption duration and wait time within a restricted subset of the Old Faithful Geyser dataset, where only short wait times (those below specified limits) are considered. We will repeat the analysis for three short wait time limits: 62, 64, and 66 minutes. By fitting a linear regression model for each of these subsets, we can evaluate whether a significant relationship exists between wait time and duration within this context.\n",
    "\n",
    "Null and Alternative Hypotheses\n",
    "Within each subset of short wait times:\n",
    "\n",
    "Null Hypothesis (\n",
    "H\n",
    "0\n",
    "H \n",
    "0\n",
    "​\t\n",
    " ): There is no linear association between waiting time and duration (i.e., the slope \n",
    "β\n",
    "1\n",
    "=\n",
    "0\n",
    "β \n",
    "1\n",
    "​\t\n",
    " =0).\n",
    "Alternative Hypothesis (\n",
    "H\n",
    "1\n",
    "H \n",
    "1\n",
    "​\t\n",
    " ): There is a linear association between waiting time and duration (i.e., \n",
    "β\n",
    "1\n",
    "≠\n",
    "0\n",
    "β \n",
    "1\n",
    "​\t\n",
    " \n",
    "\n",
    "=0).\n",
    "We will interpret the results by examining the p-value for the slope in each subset to determine whether there is evidence to reject the null hypothesis. Specifically, we are looking at how the evidence against the null hypothesis changes as we adjust the short wait limit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c009c",
   "metadata": {},
   "source": [
    "Python Code\n",
    "Here’s the code to fit and summarize the regression model for each short wait time limit and to visualize the relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Loop through each short wait limit\n",
    "for short_wait_limit in [62, 64, 66]:\n",
    "    # Filter dataset for short wait times less than the specified limit\n",
    "    short_wait = old_faithful.waiting < short_wait_limit\n",
    "    short_data = old_faithful[short_wait]\n",
    "    \n",
    "    # Fit the linear regression model\n",
    "    fitted_model = smf.ols('duration ~ waiting', data=short_data).fit()\n",
    "    print(f\"Short Wait Limit: {short_wait_limit} minutes\")\n",
    "    print(fitted_model.summary().tables[1])  # Display table of coefficients and p-values\n",
    "\n",
    "    # Create scatter plot with trendline\n",
    "    fig = px.scatter(short_data, x='waiting', y='duration', \n",
    "                     title=f\"Old Faithful Geyser Eruptions for short wait times (<{short_wait_limit} minutes)\", \n",
    "                     trendline='ols')\n",
    "    fig.show()  # Use `fig.show(renderer=\"png\")` for specific rendering requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aa07c3",
   "metadata": {},
   "source": [
    "Interpreting the Results\n",
    "For each short wait limit (62, 64, and 66 minutes), we analyze the p-value associated with the slope (wait time) in the output summary. The p-value will help us determine if there is evidence of a relationship between waiting time and duration within these subsets:\n",
    "\n",
    "If the p-value is small (e.g., < 0.05), we have evidence against the null hypothesis, suggesting a linear relationship between wait time and duration even in the restricted dataset.\n",
    "If the p-value is large (e.g., > 0.05), we do not have sufficient evidence to reject the null hypothesis, suggesting no significant linear association in this context of short wait times.\n",
    "Summary of Evidence Against the Null Hypothesis for Each Short Wait Limit\n",
    "For Short Wait Limit of 62 Minutes:\n",
    "Examine the p-value for the slope. If it’s low, this indicates evidence of a relationship between wait time and duration even within short wait times. A high p-value suggests no significant relationship.\n",
    "For Short Wait Limit of 64 Minutes:\n",
    "The evidence may change as we increase the limit. Compare the p-value with the previous result to see if the evidence for a relationship increases or decreases as the subset grows slightly larger.\n",
    "For Short Wait Limit of 66 Minutes:\n",
    "Similarly, evaluate the p-value here. Increasing the short wait limit further might show a different trend in the evidence, helping to understand whether the relationship only becomes apparent as we include longer wait times.\n",
    "Interpretation\n",
    "If we consistently find low p-values across different short wait limits, this would suggest that even within short wait times, there is a notable relationship between waiting time and duration.\n",
    "If the p-values are high for all limits, this would suggest that within short wait times, there is little evidence of a linear relationship, and the pattern observed in the full dataset might only emerge when longer wait times are included.\n",
    "By assessing the p-values across different short wait limits, we can characterize the strength of evidence for a relationship between wait time and duration specifically within the context of short wait times, as compared to the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783eacc7",
   "metadata": {},
   "source": [
    "10. Let's now consider just the (n=160) long wait times (as specified in the code below), and write code to do the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b34d5a",
   "metadata": {},
   "source": [
    "Here’s the code to perform the steps as outlined in the question. This code does the following:\n",
    "\n",
    "Creates fitted Simple Linear Regression models for bootstrap samples and collects the bootstrapped sampling distribution of the fitted slope coefficients.\n",
    "Simulates samples under the null hypothesis of \"no linear association (on average)\" and collects the sampling distribution of the fitted slope coefficients.\n",
    "Checks if the observed slope coefficient is contained within the 95% bootstrapped confidence interval and computes the simulated p-value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e74bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Load Old Faithful Geyser dataset\n",
    "old_faithful = sns.load_dataset('geyser')\n",
    "\n",
    "# Define the subset for long wait times (waiting > 63 minutes)\n",
    "long_wait = old_faithful['waiting'] > 63\n",
    "long_wait_data = old_faithful[long_wait]\n",
    "\n",
    "# Observed model for long wait times\n",
    "observed_model = smf.ols('duration ~ waiting', data=long_wait_data).fit()\n",
    "observed_slope = observed_model.params[1]\n",
    "\n",
    "# 1. Bootstrap sampling to get distribution of slope coefficients\n",
    "n_bootstrap_samples = 1000\n",
    "bootstrapped_slope_coefficients = []\n",
    "\n",
    "for _ in range(n_bootstrap_samples):\n",
    "    bootstrap_sample = long_wait_data.sample(n=len(long_wait_data), replace=True)\n",
    "    bootstrap_model = smf.ols('duration ~ waiting', data=bootstrap_sample).fit()\n",
    "    bootstrapped_slope_coefficients.append(bootstrap_model.params[1])\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "bootstrapped_slope_coefficients = np.array(bootstrapped_slope_coefficients)\n",
    "\n",
    "# Plot the bootstrapped sampling distribution of the slope coefficients\n",
    "fig1 = px.histogram(bootstrapped_slope_coefficients, nbins=30, title=\"Bootstrapped Sampling Distribution of Slope Coefficients\")\n",
    "fig1.update_layout(xaxis_title=\"Slope Coefficient\", yaxis_title=\"Frequency\")\n",
    "fig1.show()\n",
    "\n",
    "# 2. Simulated sampling distribution under the null hypothesis\n",
    "n_simulated_samples = 1000\n",
    "simulated_slope_coefficients = []\n",
    "\n",
    "# Set up null hypothesis simulation data with no linear association (slope = 0)\n",
    "old_faithful_simulation = long_wait_data.copy()\n",
    "old_faithful_simulation['duration'] = 1.65 + 0 * old_faithful_simulation['waiting'] + stats.norm(loc=0, scale=0.37).rvs(size=len(long_wait_data))\n",
    "\n",
    "for _ in range(n_simulated_samples):\n",
    "    simulated_sample = old_faithful_simulation.sample(n=len(old_faithful_simulation), replace=True)\n",
    "    simulated_model = smf.ols('duration ~ waiting', data=simulated_sample).fit()\n",
    "    simulated_slope_coefficients.append(simulated_model.params[1])\n",
    "\n",
    "# Convert to a numpy array for easier manipulation\n",
    "simulated_slope_coefficients = np.array(simulated_slope_coefficients)\n",
    "\n",
    "# Plot the simulated sampling distribution of slope coefficients under the null hypothesis\n",
    "fig2 = px.histogram(simulated_slope_coefficients, nbins=30, title=\"Simulated Sampling Distribution of Slope Coefficients (Null Hypothesis)\")\n",
    "fig2.update_layout(xaxis_title=\"Slope Coefficient\", yaxis_title=\"Frequency\")\n",
    "fig2.show()\n",
    "\n",
    "# 3. Calculate 95% bootstrapped confidence interval for the slope\n",
    "bootstrap_ci = np.quantile(bootstrapped_slope_coefficients, [0.025, 0.975])\n",
    "print(\"95% Bootstrapped Confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dda0cf",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "Bootstrap Sampling:\n",
    "We take 1000 bootstrap samples from the long wait times data.\n",
    "For each bootstrap sample, we fit a Simple Linear Regression model and collect the slope coefficient.\n",
    "We then plot the histogram of these bootstrapped slope coefficients to visualize their sampling distribution.\n",
    "Simulation Under the Null Hypothesis:\n",
    "We simulate data assuming the null hypothesis of no linear association between waiting time and duration (i.e., slope = 0).\n",
    "We add random noise to simulate the duration values based on the observed distribution (mean = 1.65, SD = 0.37) but with no dependence on waiting time.\n",
    "We take 1000 samples from this simulated dataset, fit a model for each sample, and collect the slope coefficients to create a simulated sampling distribution under the null hypothesis.\n",
    "95% Bootstrapped Confidence Interval:\n",
    "Using the bootstrapped slope coefficients, we compute the 2.5th and 97.5th percentiles to form a 95% confidence interval.\n",
    "We check if the observed slope coefficient lies within this interval.\n",
    "Simulated P-value:\n",
    "We calculate the simulated p-value under the null hypothesis by checking how many of the simulated slope coefficients are as extreme as or more extreme than the observed slope coefficient.\n",
    "This simulated p-value is then compared to the p-value obtained from the observed model fit (smf.ols(...).fit().summary().tables[1]).\n",
    "Interpretation\n",
    "Bootstrapped Confidence Interval: This interval gives an estimate of the range within which the true slope coefficient might lie, based on resampling the observed data. If the observed slope coefficient lies within this interval, it suggests that the observed data could be compatible with no effect (depending on the interval width and position).\n",
    "Simulated P-value: This p-value represents the likelihood of observing a slope coefficient as extreme as the observed one under the null hypothesis of no association. A small p-value (e.g., < 0.05) would provide strong evidence against the null hypothesis, suggesting a significant relationship between waiting time and eruption duration for long wait times.\n",
    "By analyzing both the bootstrapped confidence interval and the simulated p-value, we can assess whether there is evidence for a linear association in the subset of long wait times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a512a3",
   "metadata": {},
   "source": [
    "11. Since we've considered wait times of around <64 \"short\" and wait times of >71 \"long\", let's instead just divide the data and insead call wait times of <68 \"short\" and otherwise just call them \"long\". Consider the Simple Linear Regression model specification using an indicator variable of the wait time length\n",
    "\n",
    "\n",
    "where we use  (rather than ) (to refer to the \"kind\" or \"katagory\" or \"kontrast\") column (that you may have noticed was already a part) of the original dataset; and, explain the \"big picture\" differences between this model specification and the previously considered model specifications\n",
    "\n",
    "smf.ols('duration ~ waiting', data=old_faithful)\n",
    "smf.ols('duration ~ waiting', data=old_faithful[short_wait])\n",
    "smf.ols('duration ~ waiting', data=old_faithful[long_wait])\n",
    "and report the evidence against a null hypothesis of \"no difference between groups \"on average\") for the new indicator variable based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5bb829",
   "metadata": {},
   "source": [
    "To address this problem, let's break down the key elements and explain the differences between the previous and new model specifications. Here’s the \"big picture\" analysis and the process for testing against the null hypothesis.\n",
    "\n",
    "### 1. **Revised Definition of Wait Time Categories:**\n",
    "   - Previously, we categorized wait times as \"short\" if they were under 64 minutes and \"long\" if they were over 71 minutes.\n",
    "   - Now, we are redefining \"short\" wait times to be under 68 minutes, with anything above 68 minutes considered \"long.\"\n",
    "\n",
    "### 2. **Indicator Variable for Wait Time Length:**\n",
    "   - An indicator variable is a binary variable used to differentiate between categories—in this case, short vs. long wait times.\n",
    "   - We can define a new column, say `wait_time_length`, where:\n",
    "     - `wait_time_length = 1` for \"long\" wait times (wait time ≥ 68)\n",
    "     - `wait_time_length = 0` for \"short\" wait times (wait time < 68)\n",
    "   - We will use this indicator variable in a simple linear regression model to predict eruption `duration` based on `wait_time_length`.\n",
    "\n",
    "### 3. **New Model Specification:**\n",
    "   - The new model using the indicator variable would be specified as:\n",
    "     \\[\n",
    "     \\text{duration} \\sim \\text{wait_time_length}\n",
    "     \\]\n",
    "   - This model uses the indicator variable to separate the data into two groups (short and long wait times) and assess if there is a difference in `duration` on average between these two groups.\n",
    "\n",
    "### 4. **Comparison with Previous Models:**\n",
    "   - **Original Model (`duration ~ waiting`):** This model used the continuous `waiting` time variable to predict `duration`, assuming a linear relationship.\n",
    "   - **Separate Models for Short and Long Wait Times:** In the previous approach, separate regression models were fitted for short and long wait times:\n",
    "     - `smf.ols('duration ~ waiting', data=old_faithful[short_wait])` — a model for short waits\n",
    "     - `smf.ols('duration ~ waiting', data=old_faithful[long_wait])` — a model for long waits\n",
    "   - **New Indicator Variable Model (`duration ~ wait_time_length`):**\n",
    "     - Rather than fitting two separate models, we are now fitting a single model with an indicator variable to identify differences in eruption duration between short and long wait times.\n",
    "     - This model is simpler and can give a more straightforward interpretation of the average difference between the two groups without needing separate linear fits for each category.\n",
    "\n",
    "### 5. **\"Big Picture\" Differences Between Model Specifications:**\n",
    "   - **Complexity:** The new model with the indicator variable is simpler because it does not require fitting separate lines for each wait time category. It provides a single coefficient for the difference in means between short and long waits.\n",
    "   - **Interpretability:** The indicator model directly estimates the mean difference in `duration` between the two wait time categories, making it easier to interpret the impact of wait time length on eruption duration.\n",
    "   - **Loss of Granularity:** The previous model using `waiting` as a continuous variable captured the linear relationship across the full range of wait times, potentially providing a more nuanced fit. The indicator model loses this granularity by treating wait time as a binary variable.\n",
    "\n",
    "### 6. **Testing the Null Hypothesis of \"No Difference Between Groups\":**\n",
    "   - The null hypothesis here would be:\n",
    "     \\[\n",
    "     H_0: \\text{The coefficient for wait_time_length is zero}\n",
    "     \\]\n",
    "     This implies that there is no difference in eruption duration on average between short and long wait times.\n",
    "   - **Evidence Against the Null Hypothesis:** To test this hypothesis, we would:\n",
    "     - Fit the new model using `smf.ols('duration ~ wait_time_length', data=old_faithful)`.\n",
    "     - Examine the p-value for the coefficient of `wait_time_length`.\n",
    "     - A low p-value (typically less than 0.05) would suggest significant evidence against the null hypothesis, indicating that there is a meaningful difference in average `duration` between short and long wait times.\n",
    "\n",
    "### 7. **Conclusion:**\n",
    "   - If the p-value is significant, we can conclude that the wait time length has a significant impact on the duration of eruptions.\n",
    "   - If the p-value is not significant, it suggests that, on average, there is no substantial difference in eruption duration between the two categories of wait times under the new classification.\n",
    "\n",
    "This analysis provides a clearer understanding of the impact of wait time length on eruption duration, allowing us to compare and interpret the results more easily than with the separate regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8d18e7",
   "metadata": {},
   "source": [
    "12. Identify which of the histograms suggests the plausibility of the assumption that the distribution of error terms is normal for each of the models, and explain why the other three do not support this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3087f3cc",
   "metadata": {},
   "source": [
    "To evaluate the plausibility of the assumption that the error terms in a Simple Linear Regression model are normally distributed, we focus on the **distributional shape of the residuals**. Residuals are the differences between the observed values and the predicted values of the dependent variable.\n",
    "\n",
    "### Key Steps in the Analysis:\n",
    "1. **Normality Assumption in Regression:**\n",
    "   - The residuals (error terms) should follow a normal distribution with a mean of zero.\n",
    "   - This assumption is assessed by analyzing the histogram of the residuals. A good fit to the normality assumption will show a **bell-shaped curve**.\n",
    "\n",
    "2. **Characteristics of Histograms Supporting Normality:**\n",
    "   - The histogram of residuals should:\n",
    "     - Be symmetric around 0.\n",
    "     - Show a unimodal (single peak) and bell-shaped distribution.\n",
    "     - Have no extreme skewness or heavy tails (outliers).\n",
    "\n",
    "3. **Reasons Other Histograms Do Not Support Normality:**\n",
    "   - **Skewness:** Residuals show a distribution skewed to the left or right, indicating systematic bias in the model's predictions.\n",
    "   - **Multimodality:** Multiple peaks suggest the model fails to capture underlying patterns or relationships, which could indicate omitted variables or nonlinear effects.\n",
    "   - **Outliers:** Extreme values in the residuals violate the assumption of normality and may suggest influential points or heteroscedasticity.\n",
    "   - **Flat or Uniform Distribution:** A flat distribution implies that the residuals do not cluster near zero, which suggests that the model systematically over- or under-predicts for certain observations.\n",
    "\n",
    "4. **Diagnostic Approach (Hint from TUT):**\n",
    "   - Plot the histogram of the residuals for each model.\n",
    "   - Examine their shapes to determine if they approximate a normal distribution.\n",
    "   - Compare the histograms for symmetry, central peak, and absence of outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation of the Four Models:\n",
    "Let’s consider histograms for four different models and evaluate whether they meet the normality assumption:\n",
    "\n",
    "#### **Histogram 1:** Bell-shaped, symmetric\n",
    "- This histogram supports the normality assumption as it shows a symmetric, unimodal distribution centered around zero. \n",
    "- There are no significant outliers or skewness.\n",
    "- **Conclusion:** Plausible evidence of normally distributed residuals.\n",
    "\n",
    "#### **Histogram 2:** Skewed (e.g., right-skewed)\n",
    "- A right-skewed histogram indicates that the residuals are not symmetric, with more positive or extreme residuals. \n",
    "- This suggests systematic over- or under-prediction by the model.\n",
    "- **Conclusion:** Does not support normality.\n",
    "\n",
    "#### **Histogram 3:** Multimodal\n",
    "- A multimodal histogram suggests that the residuals cluster into distinct groups, possibly due to unmodeled subgroups or interactions in the data.\n",
    "- This could mean the model is too simple or misses important relationships.\n",
    "- **Conclusion:** Does not support normality.\n",
    "\n",
    "#### **Histogram 4:** Uniform or flat\n",
    "- A uniform distribution indicates residuals are spread evenly across the range, without clustering near zero.\n",
    "- This implies the model fails to center its predictions effectively.\n",
    "- **Conclusion:** Does not support normality.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "- Identify which histogram (likely Histogram 1) shows a bell-shaped, symmetric distribution, as this supports the assumption of normally distributed residuals.\n",
    "- The other three histograms fail to support this assumption due to skewness, multimodality, or uniformity, which indicate problems in the model such as systematic bias, omitted variables, or nonlinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad1ac1",
   "metadata": {},
   "source": [
    "13. The \"short\" and \"long\" wait times are not \"before and after\" measurements so there are not natural pairs on which to base differences on which to do a \"one sample\" (paired differences) hypothesis test; but, we can do \"two sample\" hypothesis testing using a permuation test, or create a 95% bootstrap confidence interval for the difference in means of the two populations.\n",
    "\n",
    "(A) Do a permuation test  by \"shuffling\" the labels\n",
    "\n",
    "(B) Create a 95% bootstrap confidence interval by repeatedly bootstrapping within each group and applying np.quantile(bootstrapped_mean_differences, [0.025, 0.975]) to the collection of differences between the sample means.\n",
    "\n",
    "(a) Explain how the sampling approaches work for the two simulations.\n",
    "\n",
    "(b) Compare and contrast these two methods with the indicator variable based model approach used in Question 11, explaining how they're similar and different.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91b2bb",
   "metadata": {},
   "source": [
    "### **(A) Permutation Test (Shuffling Labels):**\n",
    "\n",
    "1. **How it Works:**\n",
    "   - The **null hypothesis** assumes no difference in means between the \"short\" and \"long\" wait time groups.\n",
    "   - To simulate this, we shuffle the group labels (\"short\" and \"long\") randomly while keeping the eruption durations unchanged. This creates a **null distribution** of mean differences where any observed difference is purely due to random chance.\n",
    "   - Steps:\n",
    "     1. Calculate the observed difference in means between the two groups.\n",
    "     2. Shuffle the \"short\" and \"long\" labels and assign them to the eruption durations.\n",
    "     3. Calculate the difference in means for this randomized assignment.\n",
    "     4. Repeat the shuffling process many times (e.g., 10,000 iterations) to generate a distribution of mean differences under the null hypothesis.\n",
    "     5. Compare the observed difference to this null distribution to calculate a **p-value**: the proportion of shuffled mean differences as extreme or more extreme than the observed difference.\n",
    "\n",
    "---\n",
    "\n",
    "### **(B) Bootstrap Confidence Interval:**\n",
    "\n",
    "1. **How it Works:**\n",
    "   - The **bootstrap method** constructs confidence intervals by resampling the observed data with replacement to estimate the sampling distribution of the mean difference.\n",
    "   - Steps:\n",
    "     1. Resample the \"short\" group with replacement to create a bootstrap sample.\n",
    "     2. Resample the \"long\" group with replacement to create another bootstrap sample.\n",
    "     3. Calculate the mean difference between the two bootstrap samples.\n",
    "     4. Repeat this process many times (e.g., 10,000 iterations) to create a distribution of mean differences.\n",
    "     5. Extract the 2.5th and 97.5th percentiles of the bootstrap distribution using `np.quantile` to form a **95% confidence interval** for the mean difference.\n",
    "\n",
    "---\n",
    "\n",
    "### **(a) Explanation of Sampling Approaches:**\n",
    "\n",
    "#### **Permutation Test:**\n",
    "- **Purpose:** Tests the null hypothesis of no difference between the groups.\n",
    "- **Sampling Approach:** Randomly shuffles the labels while keeping the data fixed. This creates a distribution of mean differences under the assumption that the labels are arbitrary.\n",
    "- **Output:** A p-value quantifying how extreme the observed mean difference is compared to the null distribution.\n",
    "\n",
    "#### **Bootstrap Confidence Interval:**\n",
    "- **Purpose:** Estimates the range of plausible values for the true difference in means.\n",
    "- **Sampling Approach:** Resamples the data with replacement within each group to approximate the sampling distribution of the mean difference.\n",
    "- **Output:** A confidence interval for the mean difference, indicating the range where the true difference is likely to lie.\n",
    "\n",
    "---\n",
    "\n",
    "### **(b) Comparison with the Indicator Variable-Based Model:**\n",
    "\n",
    "#### **Similarities:**\n",
    "1. **Goal:** All methods aim to quantify the difference in eruption duration between the \"short\" and \"long\" wait time groups.\n",
    "2. **Comparison Between Groups:** All three approaches compare two distinct groups (short vs. long wait times) to evaluate whether a significant difference exists.\n",
    "3. **Underlying Assumptions:**\n",
    "   - Permutation and bootstrap methods are non-parametric and make fewer assumptions about the distribution of the data.\n",
    "   - The indicator variable approach assumes a linear relationship and normally distributed residuals.\n",
    "\n",
    "#### **Differences:**\n",
    "1. **Type of Analysis:**\n",
    "   - The **permutation test** provides a p-value to test the null hypothesis of no difference.\n",
    "   - The **bootstrap method** estimates a confidence interval for the mean difference.\n",
    "   - The **indicator variable model** estimates the mean difference directly as a regression coefficient, testing its significance via p-values.\n",
    "\n",
    "2. **Assumptions:**\n",
    "   - **Permutation test and bootstrap methods** are non-parametric and do not require normality assumptions.\n",
    "   - **Indicator variable model** requires normally distributed residuals and assumes a linear relationship between predictor and response.\n",
    "\n",
    "3. **Output:**\n",
    "   - Permutation test: p-value indicating the strength of evidence against the null hypothesis.\n",
    "   - Bootstrap: Confidence interval for the mean difference.\n",
    "   - Indicator model: Regression coefficient and p-value, along with additional model diagnostics.\n",
    "\n",
    "4. **Complexity:**\n",
    "   - The **permutation and bootstrap methods** are computationally intensive but straightforward conceptually.\n",
    "   - The **indicator variable model** involves more assumptions and interpretation but integrates the analysis within a larger regression framework.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "- The **permutation test** is ideal for hypothesis testing when we want to assess the significance of the observed difference without assuming a specific distribution.\n",
    "- The **bootstrap method** is better suited for estimating a confidence interval for the mean difference, providing a range of plausible values.\n",
    "- The **indicator variable model** provides a direct estimate of the mean difference and is useful in a regression context, but it relies on more assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fbfbe",
   "metadata": {},
   "source": [
    "14. Have you reviewed the course wiki-textbook and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f68954",
   "metadata": {},
   "source": [
    "yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38064c",
   "metadata": {},
   "source": [
    "### Summary of Our Discussion\n",
    "\n",
    "We covered the following key concepts and analyses related to **Simple Linear Regression** and its application to the Old Faithful Geyser dataset:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Theoretical Framework of Simple Linear Regression**\n",
    "- We reviewed the components of the model:\n",
    "  - \\(Y = \\beta_0 + \\beta_1 X + \\varepsilon\\), where \\(\\varepsilon \\sim N(0, \\sigma^2)\\).\n",
    "- Explained the roles of:\n",
    "  - \\(\\beta_0\\) (intercept), \\(\\beta_1\\) (slope), and the error term (\\(\\varepsilon\\)).\n",
    "- Demonstrated how this model fits data by minimizing the sum of squared residuals (Ordinary Least Squares).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Simulating and Fitting a Regression Model**\n",
    "- We simulated synthetic data to match the theoretical model and visualized:\n",
    "  - True regression line (\\(\\beta_0\\), \\(\\beta_1\\)).\n",
    "  - Fitted regression line estimated from the data.\n",
    "- Explained the differences between the true and fitted models due to **random sampling variation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Interpreting the \\(R^2\\) and Correlation**\n",
    "- \\(R^2\\) was explained as the proportion of variance in \\(Y\\) explained by the model.\n",
    "- Highlighted its equivalence to the square of the correlation coefficient (\\(r^2\\)) in Simple Linear Regression.\n",
    "- Clarified how \\(R^2\\) and \\(r^2\\) are measures of model accuracy and relationship strength.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Testing Hypotheses**\n",
    "- Formulated the null hypothesis (\\(H_0: \\beta_1 = 0\\)) for testing the existence of a linear relationship.\n",
    "- Used data from the Old Faithful Geyser to assess evidence against \\(H_0\\):\n",
    "  - \\(p\\)-values for the slope (\\(\\beta_1\\)).\n",
    "  - Visualized the regression line to inspect fit and trends.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Subgroup Analyses**\n",
    "#### Short and Long Wait Times:\n",
    "- Explored separate analyses for:\n",
    "  - Short wait times (\\(<64\\), \\(<68\\)).\n",
    "  - Long wait times (\\(>71\\), \\(>68\\)).\n",
    "- Fitted regression models to these subsets and compared results to the full dataset.\n",
    "\n",
    "#### Including an Indicator Variable:\n",
    "- Divided wait times into **short (\\(<68\\))** and **long (\\(\\geq 68\\))**.\n",
    "- Added an indicator variable to the regression model to assess differences between categories:\n",
    "  - \\(\\beta_2\\): Captured the shift in intercept for \"long\" wait times compared to \"short.\"\n",
    "  - Visualized category-specific relationships and the fitted model.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insights:\n",
    "1. **Full Dataset vs. Subsets:**\n",
    "   - Evidence of a strong linear relationship between eruption duration and waiting time in the full dataset.\n",
    "   - Subset analyses revealed possible differences in trends or strength of association for short vs. long wait times.\n",
    "\n",
    "2. **Indicator Variable:**\n",
    "   - Demonstrated how the inclusion of an indicator variable captures shifts in relationships across categories, improving interpretability.\n",
    "\n",
    "3. **Model Assumptions:**\n",
    "   - Discussed key assumptions (linearity, homoscedasticity, normality of residuals) and how data can violate them.\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like a deeper dive into any specific part of the discussion!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe723b96",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/672be0d6-f890-800b-a13a-da96e2d8964a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
